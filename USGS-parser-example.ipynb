{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing USGS QuakeML files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from xml.etree import cElementTree as ElementTree\n",
    "import urllib2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions to help parse the USGS quakeML (but are general)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Some generic utilities I use to parse the xml\n",
    "#\n",
    "#---------------------------------------------------------------------------------\n",
    "# function to search an xml item for the value specified by the key\n",
    "#   returns the value if the item is not found, the string 'None' is returned\n",
    "#   if the value is not found.\n",
    "#---------------------------------------------------------------------------------\n",
    "def get_xitem_as_text(item,key):\n",
    "    anItem = item.find(key,ns)\n",
    "    if(anItem != None):\n",
    "        return anItem.text\n",
    "    else:\n",
    "        return 'None'\n",
    "#\n",
    "#---------------------------------------------------------------------------------\n",
    "#  same type of function as above, but this one also checks that the item\n",
    "#     has a value provided.\n",
    "#---------------------------------------------------------------------------------\n",
    "def get_xitem_value_as_text(item,key,valuekey):\n",
    "    anItem = item.find(key,ns)\n",
    "    if(anItem == None):\n",
    "        return 'None'\n",
    "    else:\n",
    "        value = anItem.find(valuekey,ns)\n",
    "        if(value != None):\n",
    "            return value.text\n",
    "        else:\n",
    "            return 'None'\n",
    "#\n",
    "#---------------------------------------------------------------------------------\n",
    "def search_pdicts(key, value, list_of_dictionaries):\n",
    "    return [element for element in list_of_dictionaries if element[key] == value]\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions specifically to parse the USGS quakeML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# To make outputting information simple, I insure that certain values are in each dictionary,  \n",
    "#   whether they are defined in the xml or not. These dictionaries set up default values,\n",
    "#   but as the xml is parsed, defined key value pairs are updated.\n",
    "#\n",
    "defaultPick = {'stationCode':'--','networkCode':'--','channelCode':'--',\n",
    "                         'locationCode':'--','phase':'NA','time':'NA'}\n",
    "#\n",
    "defaultArrival = {'genericAmplitude':'NA','type':'NA','unit':'NA',\n",
    "                  'period':'NA', 'evaluationMode':'NA','timeResidual':'NA',\n",
    "                  'timeWeight':'NA'}\n",
    "#\n",
    "defaultAmplitude = {'pickID':'NA','genericAmplitude':'NA','period':'NA',\n",
    "                  'unit':'NA', 'evaluationMode':'NA'}                  \n",
    "#\n",
    "#---------------------------------------------------------------------------------\n",
    "# def getEventOrigins(xevent):\n",
    "#     xorigins = xevent.findall('d:origin',ns)\n",
    "#     return xorigins\n",
    "#\n",
    "#---------------------------------------------------------------------------------\n",
    "def parse_origins(xevent):\n",
    "    xorigins = xevent.findall('d:origin',ns)\n",
    "    origins = []\n",
    "    for xorigin in xorigins:\n",
    "        anOrigin = xorigin.attrib.copy()\n",
    "        anOrigin.update({\n",
    "        'otime': get_xitem_value_as_text(xorigin,'d:time','d:value'),\n",
    "        'latitude' : get_xitem_value_as_text(xorigin,'d:latitude','d:value'),\n",
    "        'longitude' : get_xitem_value_as_text(xorigin,'d:longitude','d:value'),\n",
    "        'depth' : get_xitem_value_as_text(xorigin,'d:depth','d:value'),\n",
    "        'dotime' : get_xitem_value_as_text(xorigin,'d:time','d:uncertainty'),\n",
    "        'dlatitude' : get_xitem_value_as_text(xorigin,'d:latitude','d:uncertainty'),\n",
    "        'dlongitude' : get_xitem_value_as_text(xorigin,'d:longitude','d:uncertainty'),\n",
    "        'ddepth' : get_xitem_value_as_text(xorigin,'d:depth','d:uncertainty')\n",
    "        })\n",
    "        #\n",
    "        origins.append(anOrigin)\n",
    "    #\n",
    "    return origins \n",
    "#\n",
    "#---------------------------------------------------------------------------------   \n",
    "def parse_magnitudes(xevent):\n",
    "    xmags = xevent.findall('d:magnitude',ns)\n",
    "    mags = []\n",
    "    for xmag in xmags:\n",
    "        mdict = xmag.attrib.copy()        \n",
    "        mdict.update({'mag': get_xitem_value_as_text(xmag,'d:mag','d:value')})       \n",
    "        mdict.update({'magType': get_xitem_as_text(xmag,'d:type')})       \n",
    "        value = get_xitem_as_text(xmag,'d:evaluationMode')\n",
    "        if(value!='NA'):\n",
    "            mdict.update({\"evaluationMode\" : value})\n",
    "            \n",
    "        value = get_xitem_as_text(xmag,'d:originID')\n",
    "        if(value!='NA'):\n",
    "            mdict.update({\"originID\" : value})\n",
    "            \n",
    "        value = get_xitem_value_as_text(xmag,'d:creationInfo', 'd:agencyID')\n",
    "        if(value!='NA'):\n",
    "            mdict.update({\"agencyID\" : value})\n",
    "        #\n",
    "        mags.append(mdict)\n",
    "    return mags\n",
    "#\n",
    "#---------------------------------------------------------------------------------\n",
    "def parse_picks(xev):\n",
    "    xpicks = xev.findall('d:pick',ns)\n",
    "    picks = []\n",
    "    for pick in xpicks:\n",
    "        pdict = defaultPick.copy()\n",
    "        pdict.update(pick.attrib.copy())\n",
    "        \n",
    "        value = get_xitem_value_as_text(pick,'d:time','d:value')\n",
    "        if(value!='NA'):\n",
    "            pdict.update({\"time\" :value})\n",
    "\n",
    "        value = get_xitem_as_text(pick,'d:phaseHint')\n",
    "        if(value!='NA'):\n",
    "            pdict.update({\"phase\" :value})\n",
    "\n",
    "        value = get_xitem_as_text(pick,'d:evaluationMode')\n",
    "        if(value!='NA'):\n",
    "            pdict.update({\"evaluationMode\" :value})\n",
    "\n",
    "        pdict.update(pick.find('d:waveformID',ns).attrib)\n",
    "        picks.append(pdict)\n",
    "    return picks\n",
    "#\n",
    "#---------------------------------------------------------------------------------\n",
    "def parse_arrivals(xorigin):\n",
    "    xarrivals = xorigin.findall('d:arrival',ns)\n",
    "    arrivals = []\n",
    "    for xarr in xarrivals:\n",
    "        adict = defaultArrival.copy()\n",
    "        value = get_xitem_as_text(xarr,'d:pickID')\n",
    "        if(value!='NA'):\n",
    "            adict.update({\"pickID\" :value})\n",
    "        value = get_xitem_as_text(xarr,'d:phase')\n",
    "        if(value!='NA'):\n",
    "            adict.update({\"phase\" :value})\n",
    "        value = get_xitem_as_text(xarr,'d:azimuth')\n",
    "        if(value!='NA'):\n",
    "            adict.update({\"azimuth\" :value})\n",
    "        value = get_xitem_as_text(xarr,'d:distance')\n",
    "        if(value!='NA'):\n",
    "            adict.update({\"distance\" :value})\n",
    "        value = get_xitem_as_text(xarr,'d:takeoffAngle')\n",
    "        if(value!='NA'):\n",
    "            adict.update({\"takeoffAngle\" :value})\n",
    "        value = get_xitem_as_text(xarr,'d:timeResidual')\n",
    "        if(value!='NA'):\n",
    "            adict.update({\"timeResidual\" :value})\n",
    "        value = get_xitem_as_text(xarr,'d:timeWeight')\n",
    "        if(value!='NA'):\n",
    "            adict.update({\"timeWeight\" :value})\n",
    "        arrivals.append(adict)\n",
    "    return arrivals    \n",
    "    \n",
    "#---------------------------------------------------------------------------------\n",
    "#  Extract the arrival items from the xml\n",
    "#\n",
    "def parse_amplitudes(xevent):\n",
    "    xamplitudes = xevent.findall('d:amplitude',ns)\n",
    "    amplitudes = []\n",
    "    for xamp in xamplitudes:\n",
    "        adict = xamp.attrib.copy()\n",
    "        adict.update(defaultAmplitude)\n",
    "\n",
    "        value = xamp.find('d:waveformID',ns)\n",
    "        if(value != None):\n",
    "            adict.update(value.attrib)\n",
    "        \n",
    "        value = get_xitem_value_as_text(xamp,'d:genericAmplitude','d:value')\n",
    "        if(value!='NA'):\n",
    "            adict.update({\"genericAmplitude\" :value})\n",
    "\n",
    "        value = get_xitem_as_text(xamp,'d:unit')\n",
    "        if(value!='NA'):\n",
    "            adict.update({\"unit\" :value})\n",
    "\n",
    "        value = get_xitem_value_as_text(xamp,'d:period','d:value')\n",
    "        if(value!='NA'):\n",
    "            adict.update({\"period\" :value})\n",
    "        \n",
    "        value = get_xitem_as_text(xamp,'d:evaluationMode')\n",
    "        if(value!='NA'):\n",
    "            adict.update({\"evaluationMode\" :value})\n",
    "        \n",
    "        value = get_xitem_as_text(xamp,'d:twindowbegin')\n",
    "        if(value!='NA'):\n",
    "            adict.update({\"twindowbegin\" :value})\n",
    "        \n",
    "        value = get_xitem_as_text(xamp,'d:twindowend')\n",
    "        if(value!='NA'):\n",
    "            adict.update({\"twindowend\" :value})\n",
    "        \n",
    "        value = get_xitem_as_text(xamp,'d:twindowref')\n",
    "        if(value!='NA'):\n",
    "            adict.update({\"twindowref\" :value})\n",
    "             \n",
    "        amplitudes.append(adict)\n",
    "\n",
    "    return amplitudes\n",
    "\n",
    "#---------------------------------------------------------------------------------\n",
    "#\n",
    "# 'distance', 'timeResidual', 'publicID', 'timeWeight', 'time', \n",
    "#     'networkCode', 'evaluationMode', 'stationCode', 'pickID', \n",
    "#     'azimuth', 'phase', 'channelCode', 'takeoffAngle', 'locationCode'\n",
    "#\n",
    "#---------------------------------------------------------------------------------\n",
    "def merge_arrivals_picks(arrivals,picks):\n",
    "    merged = []\n",
    "    for a in arrivals:\n",
    "        pid = a['pickID']\n",
    "        p = search_pdicts('publicID', pid, picks)\n",
    "        m = a.copy()\n",
    "        if(p != None):\n",
    "            m.update(p[0])\n",
    "        merged.append(m)\n",
    "    return merged\n",
    "\n",
    "#---------------------------------------------------------------------------------\n",
    "# Make a simple tab separated table of the picks with weights greater \n",
    "#    than minWeight\n",
    "def list_arrival_time_picks(arrivalTimePicks, minWeight=0.0):\n",
    "    print 'StationChannel\\tphase\\ttime\\tdistance\\tazimuth\\tResidual\\tWeight'\n",
    "    for ap in arrivalTimePicks:\n",
    "        if float(ap['timeWeight']) >= minWeight:\n",
    "            try:\n",
    "                s0 = ap['stationCode']+'-'+ap['networkCode']+'-'+ap['channelCode']+'-'+ap['locationCode']\n",
    "                s0 += '\\t'+ap['phase']+'\\t'+ap['time']\n",
    "                s0 += '\\t'+ap['distance']+'\\t'+ap['azimuth']\n",
    "                s0 += '\\t'+ap['timeResidual']+'\\t'+ap['timeWeight']\n",
    "                print s0\n",
    "            except:\n",
    "                print 'Incomplete arrival time observation.'       \n",
    "#\n",
    "#---------------------------------------------------------------------------------\n",
    "def list_magnitudes(mags):\n",
    "    print 'magType\\tagencyID\\tmagnitude'\n",
    "    for mag in mags:\n",
    "        print \"%s\\t%s\\t%s\" % (mag['magType'], mag['agencyID'],mag['mag'])\n",
    "#\n",
    "#---------------------------------------------------------------------------------\n",
    "# get the preferred origin from the eventInfo dict and the origins list\n",
    "#\n",
    "def get_preferred_origin(eventInfo,origins):\n",
    "        preforigin = eventInfo['preferredOriginID'].lower().split(\"/\")[-1]\n",
    "        for origin in origins:\n",
    "            pID = origin['publicID'].lower().split(\"/\")[-1]\n",
    "            if(pID == preforigin):\n",
    "                return origin\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Function that you use to load a file\n",
    "\n",
    "We can use the above defined functions to extract event, origin, magnitude, and phase pick information from a file containing a list of events. Each event is stored in a dictionary of dictionaries and the result is a list of events dictionaries. The origins, magnitudes, picks, amplitudes, and arrivals of the preferred origin are also extracted and stored in each event dictionary.\n",
    "\n",
    "    an_event_dictionary = {eventInfo,origins,magnitudes,picks,arrivals,amplitudes}\n",
    "\n",
    "The each dictionary is a list of key-value pairs starting with the event attributes and the list of origins (that may include origins from the travel-time location analysis and the moment-tensor estimation of faulting geometry analyses). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name spaces employed at USGS\n",
    "# need to find a way to parse these from the file.\n",
    "#\n",
    "ns = {\"q\":\"http://quakeml.org/xmlns/quakeml/1.2\",\n",
    "      \"d\":\"http://quakeml.org/xmlns/bed/1.2\",\n",
    "      \"catalog\":\"http://anss.org/xmlns/catalog/0.1\",\n",
    "      \"tensor\":\"http://anss.org/xmlns/tensor/0.1\"}\n",
    "\n",
    "def parse_usgs_xml(filepath):\n",
    "    # Import xml from online:\n",
    "    # url = 'http://earthquake.usgs.gov/realtime/product/phase-data/us20007z6r/us/1481210600040/quakeml.xml'\n",
    "    # response = urllib2.urlopen(url)\n",
    "    # xmlstring = response.read()\n",
    "    # xroot = ElementTree.fromstring(xmlstring)\n",
    "    # \n",
    "    # Import xml from a file\n",
    "    #filepath = \"20120411-OffShoreSumatra.xml\"\n",
    "    #\n",
    "    xtree = ElementTree.parse(filepath)\n",
    "    xroot = xtree.getroot()\n",
    "    #\n",
    "    xeventParameters = xroot.findall('d:eventParameters',ns)\n",
    "    #\n",
    "    for ep in xeventParameters:\n",
    "        xevents = ep.findall('d:event',ns)\n",
    "        print \"Found %d events.\" % (len(xevents))\n",
    "    #    \n",
    "    events = []\n",
    "    #\n",
    "    i = 0\n",
    "    for xev in xevents:\n",
    "        # build an event dictionary \n",
    "        ev = {}\n",
    "        ev['eventid'] = xev.attrib[\"{http://anss.org/xmlns/catalog/0.1}eventid\"]\n",
    "        ev['publicID'] = xev.attrib['publicID']\n",
    "        ev['eventsource'] = xev.attrib['{http://anss.org/xmlns/catalog/0.1}eventsource']\n",
    "        ev['datasource'] = xev.attrib['{http://anss.org/xmlns/catalog/0.1}datasource']\n",
    "        ev['preferredOriginID'] = xev.find(\"d:preferredOriginID\",ns).text\n",
    "        ev['preferredMagnitudeID'] = xev.find(\"d:preferredMagnitudeID\",ns).text\n",
    "        #\n",
    "        mags = parse_magnitudes(xev)\n",
    "        picks = parse_picks(xev)\n",
    "        amplitudes = parse_amplitudes(xev)\n",
    "        #\n",
    "        preforigin = ev['preferredOriginID'].lower().split(\"/\")[-1]\n",
    "        xorigins = xev.findall('d:origin',ns)\n",
    "        origins = parse_origins(xev)\n",
    "        pxorigin = xev[0]\n",
    "        n = 0\n",
    "        for xorigin in xorigins:\n",
    "            anOrigin = origins[n]\n",
    "            pID = anOrigin['publicID'].lower().split(\"/\")[-1]\n",
    "            if(pID == preforigin):\n",
    "                pxorigin = xorigin\n",
    "            n += 1\n",
    "        #\n",
    "        arrivals = parse_arrivals(pxorigin)\n",
    "        #\n",
    "        events.append({'eventInfo':ev,'origins':origins,'magnitudes':mags,'picks':picks,'arrivals':arrivals,'amplitudes':amplitudes})\n",
    "        #\n",
    "        i += 1\n",
    "        #\n",
    "        print \"parsed %d events.\" % (i)\n",
    "        #\n",
    "    return events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of Loading a USGS XML File\n",
    "You can use the functions to essentially convert a USGS XML file into a python dictionary. Note that I have not extracted all the information from these files (for example, I don't parse the moment tensors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE THE ARGUMENT TO THE FILE PATH\n",
    "events = parse_usgs_xml(\"20170122-SolomonIslands.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the Earthquake Information\n",
    "At this point you can begin to use the information, which is contained in a series of standard python dictionaries. I have focused on quakeML files that contain one event, but the basic building blocks can be used to process qml containing multiple events (with a little modification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_magnitudes(events[0]['magnitudes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arrivals and picks in the XML are contined in separate lists (better for operational tasks - you can have many picks but they may not all associate with an event). For most applications a seismologist would like information contained in both the pick item and the arrival item, so I merge arrivals and picks into arrivalTimePicks. Then list the information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#  print out the arrival times by merging the arrivals with the picks\n",
    "#\n",
    "arrivals = events[0]['arrivals']\n",
    "picks = events[0]['picks']\n",
    "#\n",
    "arrivalTimePicks = merge_arrivals_picks(arrivals,picks)\n",
    "#\n",
    "if(len(arrivalTimePicks) > 0):\n",
    "    list_arrival_time_picks(arrivalTimePicks,-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the Time Residual vs the Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the distance and timeResidual for data used in the location\n",
    "x = []\n",
    "y = []\n",
    "for ap in arrivalTimePicks:\n",
    "    if float(ap['timeWeight']) > 0.0:\n",
    "        x.append(ap['distance'])\n",
    "        y.append(ap['timeResidual'])\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#\n",
    "plt.plot(x,y,'ko')\n",
    "plt.axis([0, 180, -10,10])\n",
    "plt.ylabel('Travel Time Residual (s)')\n",
    "plt.xlabel('Distance (degrees)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the Time Residual vs the Azimuth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract the distance and timeResidual for data used in the location\n",
    "x = []\n",
    "y = []\n",
    "for ap in arrivalTimePicks:\n",
    "    if float(ap['timeWeight']) > 0.0:\n",
    "        x.append(ap['azimuth'])\n",
    "        y.append(ap['timeResidual'])\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#\n",
    "plt.plot(x,y,'ko')\n",
    "plt.axis([0, 180, -7,7])\n",
    "plt.ylabel('Travel Time Residual (s)')\n",
    "plt.xlabel('Azimuth (degrees)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the Travel Times\n",
    "\n",
    "To see a plot of the travel times requires that we convert the date-time strings into differences in seconds. I use python's datetime objects to handle this. The solution is not robust with time zones, but it works for UTC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "def from_utc(utcTime,fmt=\"%Y-%m-%dT%H:%M:%S.%fZ\"):\n",
    "    return datetime.datetime.strptime(utcTime, fmt)\n",
    "#\n",
    "preferredOrigin = get_preferred_origin(events[0]['eventInfo'],events[0]['origins'])\n",
    "originDateTime = from_utc(preferredOrigin['otime'])\n",
    "# Extract the distance and timeResidual for data used in the location\n",
    "x = []\n",
    "y = []\n",
    "for ap in arrivalTimePicks:\n",
    "    if float(ap['timeWeight']) > 0.0:\n",
    "        x.append(ap['distance'])\n",
    "        dt = from_utc(ap['time']) - originDateTime\n",
    "        y.append(dt.total_seconds())\n",
    "# \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#\n",
    "plt.plot(x,y,'ko')\n",
    "plt.axis([0, 180, 0, 1400])\n",
    "plt.ylabel('Travel Time (s)')\n",
    "plt.xlabel('Azimuth (degrees)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
